{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f49facf2",
   "metadata": {},
   "source": [
    "Welcome to the IrisRecognitionProject. In this short project of mine, I will be experimenting with ML models to try accurately predict the species of a iris flower based on 4 of its features: sepal length,sepal width, petal length, and petal width. There are 3 different species that we could predict: iris setosa, iris versicolor, and the iris virginica. \n",
    "Side note: The iris zip file, downloaded from the UCI ML Repository, has two different data files that differ only in 2 datapoints. This project will be using the bezdekIris file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640a511c",
   "metadata": {},
   "source": [
    "Before I can create models, I must first convert my data into a readable format for the models to understand. The only conversion I must do is to substitute the species name for a number.  I will stuff the data from the csv into a pandas dataframe, and convert the species into numerical labels using a custom mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17371b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b8a8b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepalLength  sepalWidth  petalLength  petalWidth     speciesName\n",
      "0            5.1         3.5          1.4         0.2     Iris-setosa\n",
      "1            4.9         3.0          1.4         0.2     Iris-setosa\n",
      "2            4.7         3.2          1.3         0.2     Iris-setosa\n",
      "3            4.6         3.1          1.5         0.2     Iris-setosa\n",
      "4            5.0         3.6          1.4         0.2     Iris-setosa\n",
      "..           ...         ...          ...         ...             ...\n",
      "145          6.7         3.0          5.2         2.3  Iris-virginica\n",
      "146          6.3         2.5          5.0         1.9  Iris-virginica\n",
      "147          6.5         3.0          5.2         2.0  Iris-virginica\n",
      "148          6.2         3.4          5.4         2.3  Iris-virginica\n",
      "149          5.9         3.0          5.1         1.8  Iris-virginica\n",
      "\n",
      "[150 rows x 5 columns]\n",
      "     sepalLength  sepalWidth  petalLength  petalWidth  speciesName\n",
      "0            5.1         3.5          1.4         0.2            0\n",
      "1            4.9         3.0          1.4         0.2            0\n",
      "2            4.7         3.2          1.3         0.2            0\n",
      "3            4.6         3.1          1.5         0.2            0\n",
      "4            5.0         3.6          1.4         0.2            0\n",
      "..           ...         ...          ...         ...          ...\n",
      "145          6.7         3.0          5.2         2.3            2\n",
      "146          6.3         2.5          5.0         1.9            2\n",
      "147          6.5         3.0          5.2         2.0            2\n",
      "148          6.2         3.4          5.4         2.3            2\n",
      "149          5.9         3.0          5.1         1.8            2\n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "species_mapping = {\"Iris-setosa\":0,\"Iris-versicolor\":1,\"Iris-virginica\":2}\n",
    "data = pd.read_csv('C:\\\\Users\\\\Sharvin Joshi\\\\IrisRecognitionProject\\\\iris\\\\bezdekIris.csv')\n",
    "print(data)\n",
    "data['speciesName'] = data['speciesName'].map(species_mapping)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb794409",
   "metadata": {},
   "source": [
    "Now that the labels converted into numbers, I must normalize my data to prevent outliers from heavily skewing the training and predictions. For this I will be using the simple Z score scaling method. Thankfully, the dataset is complete (no values missing), so no datapoints must be removed.\n",
    "The Z score scaling method is as follows: for each feature column X, take its mean (call it M) and standard deviation (call it S) values. Then the formula is (X-M)/S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a35237a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepalLength  sepalWidth  petalLength  petalWidth  speciesName\n",
      "0      -0.897674    1.015602    -1.335752   -1.311052            0\n",
      "1      -1.139200   -0.131539    -1.335752   -1.311052            0\n",
      "2      -1.380727    0.327318    -1.392399   -1.311052            0\n",
      "3      -1.501490    0.097889    -1.279104   -1.311052            0\n",
      "4      -1.018437    1.245030    -1.335752   -1.311052            0\n",
      "..           ...         ...          ...         ...          ...\n",
      "145     1.034539   -0.131539     0.816859    1.443994            2\n",
      "146     0.551486   -1.278680     0.703564    0.919223            2\n",
      "147     0.793012   -0.131539     0.816859    1.050416            2\n",
      "148     0.430722    0.786174     0.930154    1.443994            2\n",
      "149     0.068433   -0.131539     0.760211    0.788031            2\n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Thankfully, pandas takes care of the boring math for me with its helpful methods.\n",
    "features = ['sepalLength','sepalWidth','petalLength','petalWidth']\n",
    "for column in features:\n",
    "    featureMean = data[column].mean()\n",
    "    featureStd = data[column].std()\n",
    "    data[column]=(data[column]-featureMean)/featureStd\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e80021b",
   "metadata": {},
   "source": [
    "Now that our data has been normalized, I must split it up into training and testing sets for now.\n",
    "Also, I must have the data be shuffled before I split it since it's originally ordered by species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d86c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[features]\n",
    "y = data['speciesName']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.20, random_state=27,shuffle=True)\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "#print(X_train.size)\n",
    "#print(y_train)\n",
    "#print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca9052d",
   "metadata": {},
   "source": [
    "With our data manipulated, I can start with my models. For my first model, I want to work with nearest centroid. Since there are a small amount of classes, I'd think that finding the \"average\" of a class to determine a new datapoint's value could be an ideal way to classify.\n",
    "Since I'm not used to manually dealing with dimensions (features) greater than 3, I will be importing the NearestCentroidModel from the sklearn library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0d4df87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Score: 0.8833333333333333\n",
      "Testing Set Score: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "nearestCentroidModel = NearestCentroid()\n",
    "nearestCentroidModel.fit(X_train,y_train)\n",
    "print(\"Training Set Score:\",nearestCentroidModel.score(X_train,y_train))\n",
    "print(\"Testing Set Score:\",nearestCentroidModel.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f2669c",
   "metadata": {},
   "source": [
    "Not terrible as far as first attempts go, but since nearestCentroid can't be hypertuned all that much, this is as far as it goes. Let's try something else, like kNN, aka k Nearest Neighbors.\n",
    "\n",
    "Since we have more than 3 dimensions (features), it becomes difficult to visually plot and measure the distance between datapoints. However, knn is simple: using the training set, find the k nearest neighbors to a given datapoint using the euclidian distance formula, and return the label that appears most commonly. For funsies, I'll write this one manually..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "334ba90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier:\n",
    "    \n",
    "    def __init__(self,X_training_set,y_training_set,k=5):\n",
    "        self.k=k\n",
    "        self.X_training_set = X_training_set\n",
    "        self.y_training_set = y_training_set\n",
    "        \n",
    "    def euclidianDistance(self,point1,point2):\n",
    "        try:\n",
    "            return np.sqrt(np.sum((np.array(point1, dtype=float) - np.array(point2, dtype=float)) ** 2))\n",
    "        except ValueError:\n",
    "            return 10000000\n",
    "    \n",
    "    def predict(self,X_testing_set):\n",
    "        predictions = []\n",
    "        for datapoint in X_testing_set:\n",
    "            predicted_label = self._predict(datapoint)\n",
    "            predictions.append(predicted_label)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def _predict(self,datapoint):\n",
    "        #Get distances between datapoint and every training datapoint, find indicies of smallest distances,\n",
    "        #get corresponding labels from y_training_set, return most frequent label\n",
    "        distances = np.array([self.euclidianDistance(datapoint, training_datapoint) for training_datapoint in self.X_training_set])\n",
    "        nearestK_indicies = np.argpartition(distances, self.k)\n",
    "        corresponding_labels = [self.y_training_set[index] for index in nearestK_indicies]\n",
    "        #print(corresponding_labels)\n",
    "        return np.bincount(np.array(corresponding_labels[:self.k])).argmax()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9409c1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set Score: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "knnModel = KNNClassifier(X_train,y_train)\n",
    "knnModel_predictions = knnModel.predict(X_test)\n",
    "correct = 0\n",
    "for i in range(len(knnModel_predictions)):\n",
    "    if knnModel_predictions[i] == y_test[i]:\n",
    "        correct += 1\n",
    "print(\"Testing Set Score:\",correct/len(knnModel_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da47bda",
   "metadata": {},
   "source": [
    "While simple, the knnModel proves to be a spectacular choice for the job. The model most likely does so well because of the relatively small dataset, and the fact that it doesn't get easily swayed by outliers and anomolies. But if the k value were to become too extreme or if the dataset grew sufficently larger, I'm sure that the model would break down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57570832",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
